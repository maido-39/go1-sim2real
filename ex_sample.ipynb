{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2f6ab2da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go1PolicyInference initialized with device: cpu\n",
      "Configuration: CNN=True, RNN=False, History=9\n",
      "Initializing Go1PolicyInference...\n",
      "Actor MLP+CNN: ActorDepthCNN(\n",
      "  (prop_mlp): Sequential(\n",
      "    (0): Linear(in_features=480, out_features=512, bias=True)\n",
      "    (1): ELU(alpha=1.0)\n",
      "    (2): Linear(in_features=512, out_features=256, bias=True)\n",
      "    (3): ELU(alpha=1.0)\n",
      "    (4): Linear(in_features=256, out_features=128, bias=True)\n",
      "    (5): ELU(alpha=1.0)\n",
      "  )\n",
      "  (depth_backbone): DepthOnlyFCBackbone(\n",
      "    (image_compression): Sequential(\n",
      "      (0): Conv2d(1, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "      (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (2): ELU(alpha=1.0)\n",
      "      (3): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (5): ELU(alpha=1.0)\n",
      "      (6): Flatten(start_dim=1, end_dim=-1)\n",
      "      (7): Linear(in_features=768, out_features=256, bias=True)\n",
      "      (8): ELU(alpha=1.0)\n",
      "      (9): Linear(in_features=256, out_features=128, bias=True)\n",
      "      (10): ELU(alpha=1.0)\n",
      "    )\n",
      "  )\n",
      "  (action_head): Linear(in_features=256, out_features=12, bias=True)\n",
      ")\n",
      "Critic MLP: Sequential(\n",
      "  (0): Linear(in_features=1296, out_features=512, bias=True)\n",
      "  (1): ELU(alpha=1.0)\n",
      "  (2): Linear(in_features=512, out_features=256, bias=True)\n",
      "  (3): ELU(alpha=1.0)\n",
      "  (4): Linear(in_features=256, out_features=128, bias=True)\n",
      "  (5): ELU(alpha=1.0)\n",
      "  (6): Linear(in_features=128, out_features=1, bias=True)\n",
      ")\n",
      "Go1PolicyInference initialized successfully!\n",
      "Loading policy from ./model_14999.pt...\n",
      "Successfully loaded checkpoint from ./model_14999.pt\n",
      "Model info: {'use_cnn': True, 'has_prop_mlp': True, 'prop_mlp_input_size': 450, 'history_length': 8, 'has_depth_backbone': True, 'depth_input_channels': 16, 'num_actions': 12, 'critic_input_size': 1218}\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for ActorCriticDepthCNN:\n\tsize mismatch for actor.prop_mlp.0.weight: copying a param with shape torch.Size([512, 450]) from checkpoint, the shape in current model is torch.Size([512, 480]).\n\tsize mismatch for critic.0.weight: copying a param with shape torch.Size([512, 1218]) from checkpoint, the shape in current model is torch.Size([512, 1296]).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m inference\u001b[38;5;241m.\u001b[39minit()\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# 2. Policy 로드\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[43minference\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_policy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./model_14999.pt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# 3. Observation 업데이트\u001b[39;00m\n\u001b[1;32m     11\u001b[0m inference\u001b[38;5;241m.\u001b[39mupdate_observation(\n\u001b[1;32m     12\u001b[0m     base_ang_vel\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0.1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m0.05\u001b[39m, \u001b[38;5;241m0.02\u001b[39m],\n\u001b[1;32m     13\u001b[0m     base_rpy\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0.05\u001b[39m, \u001b[38;5;241m0.1\u001b[39m, \u001b[38;5;241m0.0\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     18\u001b[0m     depth_image\u001b[38;5;241m=\u001b[39mdepth_data\n\u001b[1;32m     19\u001b[0m )\n",
      "File \u001b[0;32m~/DeepL_WS/navila/go1_sim2real/go1_sim2real/inference.py:97\u001b[0m, in \u001b[0;36mGo1PolicyInference.load_policy\u001b[0;34m(self, policy_path)\u001b[0m\n\u001b[1;32m     94\u001b[0m         network_state_dict[key] \u001b[38;5;241m=\u001b[39m value\n\u001b[1;32m     96\u001b[0m \u001b[38;5;66;03m# Load state dict\u001b[39;00m\n\u001b[0;32m---> 97\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnetwork\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnetwork_state_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPolicy loaded successfully!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/nav/lib/python3.10/site-packages/torch/nn/modules/module.py:2153\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2148\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[1;32m   2149\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2150\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)))\n\u001b[1;32m   2152\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 2153\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2154\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n\u001b[1;32m   2155\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for ActorCriticDepthCNN:\n\tsize mismatch for actor.prop_mlp.0.weight: copying a param with shape torch.Size([512, 450]) from checkpoint, the shape in current model is torch.Size([512, 480]).\n\tsize mismatch for critic.0.weight: copying a param with shape torch.Size([512, 1218]) from checkpoint, the shape in current model is torch.Size([512, 1296])."
     ]
    }
   ],
   "source": [
    "from go1_sim2real import Go1PolicyInference\n",
    "\n",
    "# 1. 시스템 초기화\n",
    "inference = Go1PolicyInference(device=\"cpu\")\n",
    "inference.init()\n",
    "\n",
    "# 2. Policy 로드\n",
    "inference.load_policy(\"./model_14999.pt\")\n",
    "\n",
    "# 3. Observation 업데이트\n",
    "inference.update_observation(\n",
    "    base_ang_vel=[0.1, -0.05, 0.02],\n",
    "    base_rpy=[0.05, 0.1, 0.0],\n",
    "    velocity_commands=[0.5, 0.0, 0.0],\n",
    "    joint_pos=[0.0] * 12,\n",
    "    joint_vel=[0.1] * 12,\n",
    "    actions=[0.0] * 12,\n",
    "    depth_image=depth_data\n",
    ")\n",
    "\n",
    "# 4. Action 계산\n",
    "action = inference.step()\n",
    "\n",
    "# 5. 로봇 제어\n",
    "inference.apply_action()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e72f2249",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go1PolicyInference initialized with device: cpu\n",
      "Configuration: CNN=True, RNN=False, History=9\n",
      "Initializing Go1PolicyInference...\n",
      "Actor MLP+CNN: ActorDepthCNN(\n",
      "  (prop_mlp): Sequential(\n",
      "    (0): Linear(in_features=480, out_features=512, bias=True)\n",
      "    (1): ELU(alpha=1.0)\n",
      "    (2): Linear(in_features=512, out_features=256, bias=True)\n",
      "    (3): ELU(alpha=1.0)\n",
      "    (4): Linear(in_features=256, out_features=128, bias=True)\n",
      "    (5): ELU(alpha=1.0)\n",
      "  )\n",
      "  (depth_backbone): DepthOnlyFCBackbone(\n",
      "    (image_compression): Sequential(\n",
      "      (0): Conv2d(1, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "      (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (2): ELU(alpha=1.0)\n",
      "      (3): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (5): ELU(alpha=1.0)\n",
      "      (6): Flatten(start_dim=1, end_dim=-1)\n",
      "      (7): Linear(in_features=768, out_features=256, bias=True)\n",
      "      (8): ELU(alpha=1.0)\n",
      "      (9): Linear(in_features=256, out_features=128, bias=True)\n",
      "      (10): ELU(alpha=1.0)\n",
      "    )\n",
      "  )\n",
      "  (action_head): Linear(in_features=256, out_features=12, bias=True)\n",
      ")\n",
      "Critic MLP: Sequential(\n",
      "  (0): Linear(in_features=1296, out_features=512, bias=True)\n",
      "  (1): ELU(alpha=1.0)\n",
      "  (2): Linear(in_features=512, out_features=256, bias=True)\n",
      "  (3): ELU(alpha=1.0)\n",
      "  (4): Linear(in_features=256, out_features=128, bias=True)\n",
      "  (5): ELU(alpha=1.0)\n",
      "  (6): Linear(in_features=128, out_features=1, bias=True)\n",
      ")\n",
      "Go1PolicyInference initialized successfully!\n",
      "Loading policy from ./model_14999.pt...\n",
      "Successfully loaded checkpoint from ./model_14999.pt\n",
      "Model info: {'use_cnn': True, 'has_prop_mlp': True, 'prop_mlp_input_size': 450, 'history_length': 8, 'has_depth_backbone': True, 'depth_input_channels': 16, 'num_actions': 12, 'critic_input_size': 1218}\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for ActorCriticDepthCNN:\n\tsize mismatch for actor.prop_mlp.0.weight: copying a param with shape torch.Size([512, 450]) from checkpoint, the shape in current model is torch.Size([512, 480]).\n\tsize mismatch for critic.0.weight: copying a param with shape torch.Size([512, 1218]) from checkpoint, the shape in current model is torch.Size([512, 1296]).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m inference\u001b[38;5;241m.\u001b[39minit()\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# 2. Policy 로드 (수정된 코드로)\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m \u001b[43minference\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_policy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./model_14999.pt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m모델 로딩 성공!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/DeepL_WS/navila/go1_sim2real/go1_sim2real/inference.py:97\u001b[0m, in \u001b[0;36mGo1PolicyInference.load_policy\u001b[0;34m(self, policy_path)\u001b[0m\n\u001b[1;32m     94\u001b[0m         network_state_dict[key] \u001b[38;5;241m=\u001b[39m value\n\u001b[1;32m     96\u001b[0m \u001b[38;5;66;03m# Load state dict\u001b[39;00m\n\u001b[0;32m---> 97\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnetwork\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnetwork_state_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPolicy loaded successfully!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/nav/lib/python3.10/site-packages/torch/nn/modules/module.py:2153\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2148\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[1;32m   2149\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2150\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)))\n\u001b[1;32m   2152\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 2153\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2154\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n\u001b[1;32m   2155\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for ActorCriticDepthCNN:\n\tsize mismatch for actor.prop_mlp.0.weight: copying a param with shape torch.Size([512, 450]) from checkpoint, the shape in current model is torch.Size([512, 480]).\n\tsize mismatch for critic.0.weight: copying a param with shape torch.Size([512, 1218]) from checkpoint, the shape in current model is torch.Size([512, 1296])."
     ]
    }
   ],
   "source": [
    "# 수정된 코드로 테스트\n",
    "from go1_sim2real import Go1PolicyInference\n",
    "\n",
    "# 1. 시스템 초기화\n",
    "inference = Go1PolicyInference(device=\"cpu\")\n",
    "inference.init()\n",
    "\n",
    "# 2. Policy 로드 (수정된 코드로)\n",
    "inference.load_policy(\"./model_14999.pt\")\n",
    "\n",
    "print(\"모델 로딩 성공!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7139cb85",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected proprio obs dim 48, got 45",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m depth_data \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrand(\u001b[38;5;241m24\u001b[39m, \u001b[38;5;241m32\u001b[39m)\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# 3. Observation 업데이트\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m \u001b[43minference\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate_observation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbase_ang_vel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m0.05\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.02\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbase_rpy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0.05\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvelocity_commands\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mjoint_pos\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m12\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mjoint_vel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m12\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mactions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m12\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdepth_image\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdepth_data\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# 4. Action 계산\u001b[39;00m\n\u001b[1;32m     20\u001b[0m action \u001b[38;5;241m=\u001b[39m inference\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/DeepL_WS/navila/go1_sim2real/go1_sim2real/inference.py:126\u001b[0m, in \u001b[0;36mupdate_observation\u001b[0;34m(self, base_ang_vel, base_rpy, velocity_commands, joint_pos, joint_vel, actions, depth_image)\u001b[0m\n\u001b[1;32m    123\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNetwork recreated successfully!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    125\u001b[0m \u001b[38;5;66;03m# Load model state dict\u001b[39;00m\n\u001b[0;32m--> 126\u001b[0m model_state_dict \u001b[38;5;241m=\u001b[39m checkpoint[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_state_dict\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    128\u001b[0m \u001b[38;5;66;03m# Handle potential key mismatches\u001b[39;00m\n\u001b[1;32m    129\u001b[0m network_state_dict \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[0;32m~/DeepL_WS/navila/go1_sim2real/go1_sim2real/observation.py:256\u001b[0m, in \u001b[0;36mObservationManager.update_observation\u001b[0;34m(self, base_ang_vel, base_rpy, velocity_commands, joint_pos, joint_vel, actions, depth_image)\u001b[0m\n\u001b[1;32m    254\u001b[0m \u001b[38;5;66;03m# Update history buffer\u001b[39;00m\n\u001b[1;32m    255\u001b[0m proprio_obs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobs_buffer\u001b[38;5;241m.\u001b[39mget_proprio_observation()\n\u001b[0;32m--> 256\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhistory_buffer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate_history\u001b[49m\u001b[43m(\u001b[49m\u001b[43mproprio_obs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/DeepL_WS/navila/go1_sim2real/go1_sim2real/observation.py:203\u001b[0m, in \u001b[0;36mProprioHistoryBuffer.update_history\u001b[0;34m(self, proprio_obs)\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Update history buffer with new proprioceptive observation.\"\"\"\u001b[39;00m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m proprio_obs\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproprio_dim:\n\u001b[0;32m--> 203\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected proprio obs dim \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproprio_dim\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mproprio_obs\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    205\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_initialized:\n\u001b[1;32m    206\u001b[0m     \u001b[38;5;66;03m# Initialize buffer with current observation repeated\u001b[39;00m\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhistory_buffer \u001b[38;5;241m=\u001b[39m proprio_obs\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mrepeat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhistory_length, \u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: Expected proprio obs dim 48, got 45"
     ]
    }
   ],
   "source": [
    "# 더미 데이터로 전체 파이프라인 테스트\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# 더미 depth 이미지 생성 (24x32)\n",
    "depth_data = np.random.rand(24, 32).astype(np.float32)\n",
    "\n",
    "# 3. Observation 업데이트\n",
    "inference.update_observation(\n",
    "    base_ang_vel=[0.1, -0.05, 0.02],\n",
    "    base_rpy=[0.05, 0.1, 0.0],\n",
    "    velocity_commands=[0.5, 0.0, 0.0],\n",
    "    joint_pos=[0.0] * 12,\n",
    "    joint_vel=[0.1] * 12,\n",
    "    actions=[0.0] * 12,\n",
    "    depth_image=depth_data\n",
    ")\n",
    "\n",
    "# 4. Action 계산\n",
    "action = inference.step()\n",
    "print(f\"계산된 액션: {action}\")\n",
    "\n",
    "# 5. 로봇 제어 시뮬레이션\n",
    "inference.apply_action()\n",
    "\n",
    "print(\"전체 파이프라인 테스트 완료!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d75c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 체크포인트 상세 분석\n",
    "import torch\n",
    "\n",
    "# 체크포인트 로드\n",
    "checkpoint = torch.load(\"./model_14999.pt\", map_location=\"cpu\")\n",
    "\n",
    "print(\"=== 체크포인트 키 목록 ===\")\n",
    "for key in checkpoint.keys():\n",
    "    print(f\"- {key}\")\n",
    "\n",
    "print(\"\\n=== 모델 상태 딕셔너리 키 샘플 ===\")\n",
    "model_state = checkpoint[\"model_state_dict\"]\n",
    "for i, key in enumerate(model_state.keys()):\n",
    "    if i < 10:  # 처음 10개만 출력\n",
    "        print(f\"- {key}: {model_state[key].shape}\")\n",
    "    elif i == 10:\n",
    "        print(\"... (더 많은 키들)\")\n",
    "        break\n",
    "\n",
    "print(\"\\n=== 중요한 레이어들 분석 ===\")\n",
    "# Actor prop_mlp 첫 번째 레이어\n",
    "if \"actor.prop_mlp.0.weight\" in model_state:\n",
    "    prop_weight = model_state[\"actor.prop_mlp.0.weight\"]\n",
    "    print(f\"Actor prop_mlp 첫 번째 레이어: {prop_weight.shape}\")\n",
    "    print(f\"  입력 차원: {prop_weight.shape[1]}\")\n",
    "    print(f\"  출력 차원: {prop_weight.shape[0]}\")\n",
    "\n",
    "# Critic 첫 번째 레이어\n",
    "if \"critic.0.weight\" in model_state:\n",
    "    critic_weight = model_state[\"critic.0.weight\"]\n",
    "    print(f\"Critic 첫 번째 레이어: {critic_weight.shape}\")\n",
    "    print(f\"  입력 차원: {critic_weight.shape[1]}\")\n",
    "    print(f\"  출력 차원: {critic_weight.shape[0]}\")\n",
    "\n",
    "# Action head\n",
    "if \"actor.action_head.weight\" in model_state:\n",
    "    action_weight = model_state[\"actor.action_head.weight\"]\n",
    "    print(f\"Actor action head: {action_weight.shape}\")\n",
    "    print(f\"  입력 차원: {action_weight.shape[1]}\")\n",
    "    print(f\"  출력 차원: {action_weight.shape[0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d47ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 히스토리 길이 계산 로직 분석\n",
    "print(\"=== 히스토리 길이 계산 분석 ===\")\n",
    "\n",
    "# 체크포인트에서 실제 값들\n",
    "prop_mlp_input_size = 450\n",
    "critic_input_size = 1218\n",
    "num_proprio_obs = 48  # 기본 proprioceptive observation 차원\n",
    "\n",
    "print(f\"체크포인트에서:\")\n",
    "print(f\"  prop_mlp_input_size: {prop_mlp_input_size}\")\n",
    "print(f\"  critic_input_size: {critic_input_size}\")\n",
    "print(f\"  num_proprio_obs: {num_proprio_obs}\")\n",
    "\n",
    "# 현재 계산 방식 (잘못된 방식)\n",
    "current_history_length = (prop_mlp_input_size // num_proprio_obs) - 1\n",
    "print(f\"\\n현재 계산 방식:\")\n",
    "print(f\"  history_length = (prop_mlp_input_size // num_proprio_obs) - 1\")\n",
    "print(f\"  history_length = ({prop_mlp_input_size} // {num_proprio_obs}) - 1 = {current_history_length}\")\n",
    "\n",
    "# 실제로는 어떻게 계산되어야 하는지 확인\n",
    "print(f\"\\n실제 계산:\")\n",
    "print(f\"  prop_mlp_input_size = {prop_mlp_input_size}\")\n",
    "print(f\"  num_proprio_obs = {num_proprio_obs}\")\n",
    "print(f\"  prop_mlp_input_size / num_proprio_obs = {prop_mlp_input_size / num_proprio_obs}\")\n",
    "print(f\"  정수 나눗셈: {prop_mlp_input_size // num_proprio_obs}\")\n",
    "print(f\"  나머지: {prop_mlp_input_size % num_proprio_obs}\")\n",
    "\n",
    "# legged-loco의 실제 계산 방식 확인\n",
    "print(f\"\\nlegged-loco 방식:\")\n",
    "print(f\"  num_actor_obs_prop = num_actor_obs_prop * (history_length + 1)\")\n",
    "print(f\"  따라서: history_length = (num_actor_obs_prop / num_proprio_obs) - 1\")\n",
    "print(f\"  하지만 실제로는: {prop_mlp_input_size} / {num_proprio_obs} = {prop_mlp_input_size / num_proprio_obs}\")\n",
    "\n",
    "# 정확한 히스토리 길이 계산\n",
    "if prop_mlp_input_size % num_proprio_obs == 0:\n",
    "    correct_history_length = (prop_mlp_input_size // num_proprio_obs) - 1\n",
    "    print(f\"\\n정확한 히스토리 길이: {correct_history_length}\")\n",
    "else:\n",
    "    print(f\"\\n⚠️  나머지가 있음! 정확한 계산이 필요함\")\n",
    "    print(f\"  450 / 48 = 9.375\")\n",
    "    print(f\"  이는 히스토리 길이가 정수가 아님을 의미\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8044c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 근본 원인 분석: 실제 학습 설정과 체크포인트 불일치\n",
    "print(\"=== 근본 원인 분석 ===\")\n",
    "\n",
    "# 1. 학습 시 설정 (StudyNote.md에서 확인)\n",
    "print(\"1. 학습 시 설정:\")\n",
    "print(\"   --history_length=9\")\n",
    "print(\"   --use_cnn\")\n",
    "print(\"   --enable_cameras\")\n",
    "\n",
    "# 2. 체크포인트에서 실제 값\n",
    "print(\"\\n2. 체크포인트에서 실제 값:\")\n",
    "print(\"   prop_mlp_input_size: 450\")\n",
    "print(\"   critic_input_size: 1218\")\n",
    "\n",
    "# 3. 계산 분석\n",
    "print(\"\\n3. 계산 분석:\")\n",
    "print(\"   만약 history_length=9라면:\")\n",
    "print(\"   num_actor_obs_prop = 48 * (9 + 1) = 48 * 10 = 480\")\n",
    "print(\"   하지만 실제로는 450\")\n",
    "\n",
    "print(\"\\n   만약 history_length=8라면:\")\n",
    "print(\"   num_actor_obs_prop = 48 * (8 + 1) = 48 * 9 = 432\")\n",
    "print(\"   하지만 실제로는 450\")\n",
    "\n",
    "print(\"\\n   실제 계산:\")\n",
    "print(\"   450 / 48 = 9.375\")\n",
    "print(\"   이는 정수가 아님!\")\n",
    "\n",
    "# 4. 가능한 원인들\n",
    "print(\"\\n4. 가능한 원인들:\")\n",
    "print(\"   a) 학습 시 실제로는 다른 history_length가 사용됨\")\n",
    "print(\"   b) proprioceptive observation 차원이 48이 아님\")\n",
    "print(\"   c) 히스토리 버퍼링 방식이 다름\")\n",
    "print(\"   d) 체크포인트가 다른 설정으로 학습됨\")\n",
    "\n",
    "# 5. 실제 proprioceptive observation 차원 확인\n",
    "print(\"\\n5. 실제 proprioceptive observation 차원 확인:\")\n",
    "print(\"   450을 나누어떨어지게 하는 수들:\")\n",
    "for i in range(1, 50):\n",
    "    if 450 % i == 0:\n",
    "        print(f\"   {i}: 450 / {i} = {450 // i}\")\n",
    "        if 450 // i - 1 == 9:\n",
    "            print(f\"   → {i}개 차원, history_length=9일 때 가능!\")\n",
    "\n",
    "print(\"\\n6. 결론:\")\n",
    "print(\"   체크포인트는 history_length=9로 학습되지 않았거나,\")\n",
    "print(\"   proprioceptive observation 차원이 48이 아닐 가능성이 높음\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe2d520e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 실제 학습 설정 확인\n",
    "print(\"=== 실제 학습 설정 확인 ===\")\n",
    "\n",
    "# agent.yaml에서 확인한 설정\n",
    "print(\"agent.yaml에서 확인한 설정:\")\n",
    "print(\"  history_length: 9\")\n",
    "print(\"  use_cnn: true\")\n",
    "print(\"  obs_depth_shape: [24, 32]\")\n",
    "print(\"  actor_hidden_dims: [512, 256, 128]\")\n",
    "print(\"  critic_hidden_dims: [512, 256, 128]\")\n",
    "\n",
    "# 하지만 체크포인트에서는 다른 값\n",
    "print(\"\\n체크포인트에서 실제 값:\")\n",
    "print(\"  prop_mlp_input_size: 450\")\n",
    "print(\"  critic_input_size: 1218\")\n",
    "\n",
    "# 이제 정확한 계산을 해보자\n",
    "print(\"\\n=== 정확한 계산 ===\")\n",
    "\n",
    "# 450을 나누어떨어지게 하는 수들 중에서 history_length=9가 가능한지 확인\n",
    "print(\"450의 약수들:\")\n",
    "for i in range(1, 51):\n",
    "    if 450 % i == 0:\n",
    "        history_length = (450 // i) - 1\n",
    "        print(f\"  proprio_dim={i}: history_length={history_length}\")\n",
    "        if history_length == 9:\n",
    "            print(f\"    → 이게 정답! proprio_dim={i}, history_length=9\")\n",
    "\n",
    "# 1218도 확인해보자\n",
    "print(\"\\n1218의 약수들 (critic input):\")\n",
    "for i in range(1, 51):\n",
    "    if 1218 % i == 0:\n",
    "        print(f\"  {i}: 1218 / {i} = {1218 // i}\")\n",
    "\n",
    "print(\"\\n=== 결론 ===\")\n",
    "print(\"실제 학습된 모델은:\")\n",
    "print(\"  proprioceptive observation 차원: 45 (48이 아님!)\")\n",
    "print(\"  history_length: 9\")\n",
    "print(\"  따라서: 45 * (9 + 1) = 45 * 10 = 450\")\n",
    "print(\"  critic input: 1218 (정확한 계산 필요)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db476279",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 최종 해결책: 올바른 설정으로 네트워크 재생성\n",
    "print(\"=== 최종 해결책 ===\")\n",
    "\n",
    "# 실제 학습된 모델의 정확한 설정\n",
    "actual_proprio_dim = 45  # 450 / 10 = 45\n",
    "actual_history_length = 9\n",
    "actual_prop_mlp_input = 450  # 45 * 10 = 450\n",
    "\n",
    "print(f\"실제 학습된 모델 설정:\")\n",
    "print(f\"  proprioceptive observation 차원: {actual_proprio_dim}\")\n",
    "print(f\"  history_length: {actual_history_length}\")\n",
    "print(f\"  prop_mlp_input_size: {actual_prop_mlp_input}\")\n",
    "\n",
    "# 우리 코드에서 수정해야 할 부분\n",
    "print(f\"\\n우리 코드에서 수정해야 할 부분:\")\n",
    "print(f\"  config.py의 num_proprio_obs: 48 → {actual_proprio_dim}\")\n",
    "print(f\"  또는 체크포인트에서 자동으로 감지하도록 수정\")\n",
    "\n",
    "# utils.py의 get_model_info_from_checkpoint 함수 수정 필요\n",
    "print(f\"\\nutils.py 수정 방향:\")\n",
    "print(f\"  1. 체크포인트에서 prop_mlp_input_size 추출\")\n",
    "print(f\"  2. 가능한 proprio_dim과 history_length 조합 계산\")\n",
    "print(f\"  3. 가장 합리적인 조합 선택 (history_length=9 우선)\")\n",
    "\n",
    "# 실제 계산 예시\n",
    "print(f\"\\n실제 계산 예시:\")\n",
    "print(f\"  prop_mlp_input_size = {actual_prop_mlp_input}\")\n",
    "print(f\"  가능한 조합들:\")\n",
    "for proprio_dim in range(1, 51):\n",
    "    if actual_prop_mlp_input % proprio_dim == 0:\n",
    "        history_length = (actual_prop_mlp_input // proprio_dim) - 1\n",
    "        print(f\"    proprio_dim={proprio_dim}, history_length={history_length}\")\n",
    "        if history_length == 9:\n",
    "            print(f\"      → 정답! proprio_dim={proprio_dim}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f9997b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 수정된 코드로 최종 테스트\n",
    "print(\"=== 수정된 코드로 최종 테스트 ===\")\n",
    "\n",
    "# 기존 inference 객체 제거\n",
    "if 'inference' in locals():\n",
    "    del inference\n",
    "\n",
    "# 새로운 inference 객체 생성\n",
    "from go1_sim2real import Go1PolicyInference\n",
    "\n",
    "# 1. 시스템 초기화\n",
    "inference = Go1PolicyInference(device=\"cpu\")\n",
    "inference.init()\n",
    "\n",
    "# 2. Policy 로드 (수정된 자동 감지 로직 사용)\n",
    "print(\"\\n=== Policy 로딩 (수정된 로직) ===\")\n",
    "inference.load_policy(\"./model_14999.pt\")\n",
    "\n",
    "print(\"\\n✅ 모델 로딩 성공!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e862d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정확한 proprioceptive observation 차원 계산\n",
    "print(\"=== 정확한 계산 ===\")\n",
    "\n",
    "# legged-loco Go1 vision 설정에서 확인한 관측값들\n",
    "print(\"Proprioceptive observation 구성요소:\")\n",
    "print(\"  base_lin_vel: 3차원\")\n",
    "print(\"  base_ang_vel: 3차원\") \n",
    "print(\"  projected_gravity: 3차원\")\n",
    "print(\"  base_rpy: 3차원\")\n",
    "print(\"  velocity_commands: 3차원\")\n",
    "print(\"  joint_pos: 12차원\")\n",
    "print(\"  joint_vel: 12차원\")\n",
    "print(\"  actions: 12차원\")\n",
    "\n",
    "# 정확한 계산\n",
    "total_proprio_dim = 3 + 3 + 3 + 3 + 3 + 12 + 12 + 12\n",
    "print(f\"\\n정확한 계산:\")\n",
    "print(f\"  3 + 3 + 3 + 3 + 3 + 12 + 12 + 12 = {total_proprio_dim}\")\n",
    "\n",
    "print(f\"\\n우리 코드의 잘못된 계산:\")\n",
    "print(f\"  base_ang_vel(3) + base_rpy(3) + velocity_commands(3) + joint_pos(12) + joint_vel(12) + actions(12)\")\n",
    "print(f\"  = 3 + 3 + 3 + 12 + 12 + 12 = 45\")\n",
    "print(f\"  하지만 주석에는 + depth_image(768) - depth_image(768) = 48 이라고 잘못 표기됨\")\n",
    "\n",
    "print(f\"\\n실제로는:\")\n",
    "print(f\"  proprioceptive observation 차원: {total_proprio_dim}\")\n",
    "print(f\"  history_length: 9\")\n",
    "print(f\"  prop_mlp_input_size: {total_proprio_dim} * (9 + 1) = {total_proprio_dim * 10}\")\n",
    "\n",
    "print(f\"\\n체크포인트와 비교:\")\n",
    "print(f\"  체크포인트 prop_mlp_input_size: 450\")\n",
    "print(f\"  계산된 값: {total_proprio_dim * 10}\")\n",
    "print(f\"  일치 여부: {450 == total_proprio_dim * 10}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "215c1d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 다시 정확히 계산해보자\n",
    "print(\"=== 다시 정확히 계산 ===\")\n",
    "\n",
    "# 체크포인트에서 실제 값\n",
    "checkpoint_prop_mlp_input = 450\n",
    "checkpoint_history_length = 9\n",
    "\n",
    "print(f\"체크포인트에서:\")\n",
    "print(f\"  prop_mlp_input_size: {checkpoint_prop_mlp_input}\")\n",
    "print(f\"  history_length: {checkpoint_history_length}\")\n",
    "\n",
    "# 역산으로 실제 proprioceptive observation 차원 계산\n",
    "actual_proprio_dim = checkpoint_prop_mlp_input // (checkpoint_history_length + 1)\n",
    "print(f\"\\n역산 계산:\")\n",
    "print(f\"  actual_proprio_dim = {checkpoint_prop_mlp_input} // ({checkpoint_history_length} + 1)\")\n",
    "print(f\"  actual_proprio_dim = {checkpoint_prop_mlp_input} // 10 = {actual_proprio_dim}\")\n",
    "\n",
    "print(f\"\\n실제 legged-loco에서 사용된 관측값들:\")\n",
    "print(f\"  base_lin_vel: 3차원\")\n",
    "print(f\"  base_ang_vel: 3차원\") \n",
    "print(f\"  projected_gravity: 3차원\")\n",
    "print(f\"  base_rpy: 3차원\")\n",
    "print(f\"  velocity_commands: 3차원\")\n",
    "print(f\"  joint_pos: 12차원\")\n",
    "print(f\"  joint_vel: 12차원\")\n",
    "print(f\"  actions: 12차원\")\n",
    "print(f\"  총합: 3+3+3+3+3+12+12+12 = 51차원\")\n",
    "\n",
    "print(f\"\\n하지만 실제로는 45차원이 사용됨!\")\n",
    "print(f\"이는 일부 관측값이 제외되었거나 다른 구성일 가능성\")\n",
    "\n",
    "# 45차원이 되려면 어떤 조합이 가능한지 확인\n",
    "print(f\"\\n45차원이 되려면:\")\n",
    "print(f\"  3 + 3 + 3 + 3 + 3 + 12 + 12 + 6 = 45 (actions가 6차원?)\")\n",
    "print(f\"  또는\")\n",
    "print(f\"  3 + 3 + 3 + 3 + 3 + 12 + 12 + 6 = 45 (다른 조합)\")\n",
    "\n",
    "print(f\"\\n결론: 실제 학습된 모델은 45차원 proprioceptive observation 사용\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "997d996e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 수정된 config.py로 최종 테스트\n",
    "print(\"=== 수정된 config.py로 최종 테스트 ===\")\n",
    "\n",
    "# 기존 inference 객체 제거\n",
    "if 'inference' in locals():\n",
    "    del inference\n",
    "\n",
    "# 새로운 inference 객체 생성\n",
    "from go1_sim2real import Go1PolicyInference\n",
    "\n",
    "# 1. 시스템 초기화\n",
    "inference = Go1PolicyInference(device=\"cpu\")\n",
    "inference.init()\n",
    "\n",
    "print(f\"현재 설정:\")\n",
    "print(f\"  num_proprio_obs: {inference.config.num_proprio_obs}\")\n",
    "print(f\"  history_length: {inference.config.history_length}\")\n",
    "print(f\"  num_actor_obs_prop: {inference.config.num_actor_obs_prop}\")\n",
    "print(f\"  total_policy_input_dim: {inference.config.total_policy_input_dim}\")\n",
    "\n",
    "# 2. Policy 로드\n",
    "print(\"\\n=== Policy 로딩 ===\")\n",
    "inference.load_policy(\"./model_14999.pt\")\n",
    "\n",
    "print(\"\\n✅ 모델 로딩 성공!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "68432291",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 수정된 utils.py로 최종 테스트 ===\n",
      "Go1PolicyInference initialized with device: cpu\n",
      "Configuration: CNN=True, RNN=False, History=9\n",
      "Initializing Go1PolicyInference...\n",
      "Actor MLP+CNN: ActorDepthCNN(\n",
      "  (prop_mlp): Sequential(\n",
      "    (0): Linear(in_features=480, out_features=512, bias=True)\n",
      "    (1): ELU(alpha=1.0)\n",
      "    (2): Linear(in_features=512, out_features=256, bias=True)\n",
      "    (3): ELU(alpha=1.0)\n",
      "    (4): Linear(in_features=256, out_features=128, bias=True)\n",
      "    (5): ELU(alpha=1.0)\n",
      "  )\n",
      "  (depth_backbone): DepthOnlyFCBackbone(\n",
      "    (image_compression): Sequential(\n",
      "      (0): Conv2d(1, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "      (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (2): ELU(alpha=1.0)\n",
      "      (3): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (5): ELU(alpha=1.0)\n",
      "      (6): Flatten(start_dim=1, end_dim=-1)\n",
      "      (7): Linear(in_features=768, out_features=256, bias=True)\n",
      "      (8): ELU(alpha=1.0)\n",
      "      (9): Linear(in_features=256, out_features=128, bias=True)\n",
      "      (10): ELU(alpha=1.0)\n",
      "    )\n",
      "  )\n",
      "  (action_head): Linear(in_features=256, out_features=12, bias=True)\n",
      ")\n",
      "Critic MLP: Sequential(\n",
      "  (0): Linear(in_features=1296, out_features=512, bias=True)\n",
      "  (1): ELU(alpha=1.0)\n",
      "  (2): Linear(in_features=512, out_features=256, bias=True)\n",
      "  (3): ELU(alpha=1.0)\n",
      "  (4): Linear(in_features=256, out_features=128, bias=True)\n",
      "  (5): ELU(alpha=1.0)\n",
      "  (6): Linear(in_features=128, out_features=1, bias=True)\n",
      ")\n",
      "Go1PolicyInference initialized successfully!\n",
      "초기 설정:\n",
      "  num_proprio_obs: 48\n",
      "  history_length: 9\n",
      "  num_actor_obs_prop: 480\n",
      "  total_policy_input_dim: 1296\n",
      "\n",
      "=== Policy 로딩 (수정된 로직) ===\n",
      "Loading policy from ./model_14999.pt...\n",
      "Successfully loaded checkpoint from ./model_14999.pt\n",
      "Model info: {'use_cnn': True, 'has_prop_mlp': True, 'prop_mlp_input_size': 450, 'history_length': 8, 'has_depth_backbone': True, 'depth_input_channels': 16, 'num_actions': 12, 'critic_input_size': 1218}\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for ActorCriticDepthCNN:\n\tsize mismatch for actor.prop_mlp.0.weight: copying a param with shape torch.Size([512, 450]) from checkpoint, the shape in current model is torch.Size([512, 480]).\n\tsize mismatch for critic.0.weight: copying a param with shape torch.Size([512, 1218]) from checkpoint, the shape in current model is torch.Size([512, 1296]).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 23\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# 2. Policy 로드 (수정된 자동 감지 로직 사용)\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m=== Policy 로딩 (수정된 로직) ===\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 23\u001b[0m \u001b[43minference\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_policy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./model_14999.pt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m최종 설정:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  num_proprio_obs: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minference\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_proprio_obs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/DeepL_WS/navila/go1_sim2real/go1_sim2real/inference.py:97\u001b[0m, in \u001b[0;36mGo1PolicyInference.load_policy\u001b[0;34m(self, policy_path)\u001b[0m\n\u001b[1;32m     94\u001b[0m         network_state_dict[key] \u001b[38;5;241m=\u001b[39m value\n\u001b[1;32m     96\u001b[0m \u001b[38;5;66;03m# Load state dict\u001b[39;00m\n\u001b[0;32m---> 97\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnetwork\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnetwork_state_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPolicy loaded successfully!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/nav/lib/python3.10/site-packages/torch/nn/modules/module.py:2153\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2148\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[1;32m   2149\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2150\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)))\n\u001b[1;32m   2152\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 2153\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2154\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n\u001b[1;32m   2155\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for ActorCriticDepthCNN:\n\tsize mismatch for actor.prop_mlp.0.weight: copying a param with shape torch.Size([512, 450]) from checkpoint, the shape in current model is torch.Size([512, 480]).\n\tsize mismatch for critic.0.weight: copying a param with shape torch.Size([512, 1218]) from checkpoint, the shape in current model is torch.Size([512, 1296])."
     ]
    }
   ],
   "source": [
    "# 수정된 utils.py로 최종 테스트\n",
    "print(\"=== 수정된 utils.py로 최종 테스트 ===\")\n",
    "\n",
    "# 기존 inference 객체 제거\n",
    "if 'inference' in locals():\n",
    "    del inference\n",
    "\n",
    "# 새로운 inference 객체 생성\n",
    "from go1_sim2real import Go1PolicyInference\n",
    "\n",
    "# 1. 시스템 초기화\n",
    "inference = Go1PolicyInference(device=\"cpu\")\n",
    "inference.init()\n",
    "\n",
    "print(f\"초기 설정:\")\n",
    "print(f\"  num_proprio_obs: {inference.config.num_proprio_obs}\")\n",
    "print(f\"  history_length: {inference.config.history_length}\")\n",
    "print(f\"  num_actor_obs_prop: {inference.config.num_actor_obs_prop}\")\n",
    "print(f\"  total_policy_input_dim: {inference.config.total_policy_input_dim}\")\n",
    "\n",
    "# 2. Policy 로드 (수정된 자동 감지 로직 사용)\n",
    "print(\"\\n=== Policy 로딩 (수정된 로직) ===\")\n",
    "inference.load_policy(\"./model_14999.pt\")\n",
    "\n",
    "print(f\"\\n최종 설정:\")\n",
    "print(f\"  num_proprio_obs: {inference.config.num_proprio_obs}\")\n",
    "print(f\"  history_length: {inference.config.history_length}\")\n",
    "print(f\"  num_actor_obs_prop: {inference.config.num_actor_obs_prop}\")\n",
    "print(f\"  total_policy_input_dim: {inference.config.total_policy_input_dim}\")\n",
    "\n",
    "print(\"\\n✅ 모델 로딩 성공!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1027407e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전체 파이프라인 테스트\n",
    "print(\"=== 전체 파이프라인 테스트 ===\")\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# 더미 depth 이미지 생성 (24x32)\n",
    "depth_data = np.random.rand(24, 32).astype(np.float32)\n",
    "\n",
    "# 3. Observation 업데이트\n",
    "print(\"Observation 업데이트 중...\")\n",
    "inference.update_observation(\n",
    "    base_ang_vel=[0.1, -0.05, 0.02],\n",
    "    base_rpy=[0.05, 0.1, 0.0],\n",
    "    velocity_commands=[0.5, 0.0, 0.0],\n",
    "    joint_pos=[0.0] * 12,\n",
    "    joint_vel=[0.1] * 12,\n",
    "    actions=[0.0] * 12,\n",
    "    depth_image=depth_data\n",
    ")\n",
    "\n",
    "# 4. Action 계산\n",
    "print(\"Action 계산 중...\")\n",
    "action = inference.step()\n",
    "print(f\"계산된 액션: {action}\")\n",
    "\n",
    "# 5. 로봇 제어 시뮬레이션\n",
    "print(\"로봇 제어 시뮬레이션...\")\n",
    "inference.apply_action()\n",
    "\n",
    "print(\"\\n🎉 전체 파이프라인 테스트 완료!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e88f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 최종 수정된 코드로 테스트\n",
    "print(\"=== 최종 수정된 코드로 테스트 ===\")\n",
    "\n",
    "# 기존 inference 객체 제거\n",
    "if 'inference' in locals():\n",
    "    del inference\n",
    "\n",
    "# 새로운 inference 객체 생성\n",
    "from go1_sim2real import Go1PolicyInference\n",
    "\n",
    "# 1. 시스템 초기화\n",
    "inference = Go1PolicyInference(device=\"cpu\")\n",
    "inference.init()\n",
    "\n",
    "print(f\"초기 설정:\")\n",
    "print(f\"  num_proprio_obs: {inference.config.num_proprio_obs}\")\n",
    "print(f\"  history_length: {inference.config.history_length}\")\n",
    "print(f\"  num_actor_obs_prop: {inference.config.num_actor_obs_prop}\")\n",
    "print(f\"  total_policy_input_dim: {inference.config.total_policy_input_dim}\")\n",
    "\n",
    "# 2. Policy 로드 (체크포인트에서 감지된 설정 사용)\n",
    "print(\"\\n=== Policy 로딩 (체크포인트 기반 설정) ===\")\n",
    "inference.load_policy(\"./model_14999.pt\")\n",
    "\n",
    "print(f\"\\n최종 설정:\")\n",
    "print(f\"  num_proprio_obs: {inference.config.num_proprio_obs}\")\n",
    "print(f\"  history_length: {inference.config.history_length}\")\n",
    "print(f\"  num_actor_obs_prop: {inference.config.num_actor_obs_prop}\")\n",
    "print(f\"  total_policy_input_dim: {inference.config.total_policy_input_dim}\")\n",
    "\n",
    "print(\"\\n✅ 모델 로딩 성공!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f184f264",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전체 파이프라인 최종 테스트\n",
    "print(\"=== 전체 파이프라인 최종 테스트 ===\")\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# 더미 depth 이미지 생성 (24x32)\n",
    "depth_data = np.random.rand(24, 32).astype(np.float32)\n",
    "\n",
    "# 3. Observation 업데이트\n",
    "print(\"Observation 업데이트 중...\")\n",
    "inference.update_observation(\n",
    "    base_ang_vel=[0.1, -0.05, 0.02],\n",
    "    base_rpy=[0.05, 0.1, 0.0],\n",
    "    velocity_commands=[0.5, 0.0, 0.0],\n",
    "    joint_pos=[0.0] * 12,\n",
    "    joint_vel=[0.1] * 12,\n",
    "    actions=[0.0] * 12,\n",
    "    depth_image=depth_data\n",
    ")\n",
    "\n",
    "# 4. Action 계산\n",
    "print(\"Action 계산 중...\")\n",
    "action = inference.step()\n",
    "print(f\"계산된 액션: {action}\")\n",
    "\n",
    "# 5. 로봇 제어 시뮬레이션\n",
    "print(\"로봇 제어 시뮬레이션...\")\n",
    "inference.apply_action()\n",
    "\n",
    "print(\"\\n🎉 전체 파이프라인 테스트 완료!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe14f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 간단한 정책 테스트 스크립트 실행\n",
    "print(\"=== 간단한 정책 테스트 ===\")\n",
    "\n",
    "# 간단한 스크립트 실행\n",
    "exec(open('simple_policy_test.py').read())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef561cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 초간단 정책 테스트\n",
    "print(\"=== 초간단 정책 테스트 ===\")\n",
    "\n",
    "# 초간단 스크립트 실행\n",
    "exec(open('ultra_simple_policy.py').read())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69bb4e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f9aae77",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1837bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "334bf9fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b1ff79",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9519c35f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba81060",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nav",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
